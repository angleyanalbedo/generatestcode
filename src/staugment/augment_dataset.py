import copy
import json
from pathlib import Path

from tqdm import tqdm

from ..stparser import STParser
from ..stanalyzer.lark_analyzer import STSemanticAnalyzer
from ..stunparser.unparser import STUnparser
from ..strewriter import STRewriter
from ..utils import auto_repair


class DataAugmenter:
    def __init__(self, input_dir: str, output_dir: str, ext: str = ".json", num_variants: int = 2):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.ext = ext
        self.num_variants = num_variants

        # åˆå§‹åŒ–æ ¸å¿ƒå¼•æ“
        self.parser = STParser()
        self.analyzer = STSemanticAnalyzer()
        self.rewriter = STRewriter(analyzer=self.analyzer, mode="augment")
        self.unparser = STUnparser()

        # æ‰¹å¤„ç†ç»Ÿè®¡
        self.stats = {
            "total_files": 0,
            "processed_files": 0,
            "total_original": 0,
            "total_augmented": 0,
            "parse_errors": 0
        }

    def process_single_file(self, file_path: Path) -> list:
        """å¤„ç†å•ä¸ª JSON æ–‡ä»¶ï¼Œè¿”å›åŒ…å«äº†åŸæ•°æ®å’Œå¢å¼ºæ•°æ®çš„æ··åˆåˆ—è¡¨"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
        except Exception as e:
            print(f"\nâš ï¸ è­¦å‘Š: æ–‡ä»¶è¯»å–å¤±è´¥ -> {file_path.name}: {e}")
            return []

        if not isinstance(dataset, list):
            return []

        augmented_dataset = []

        for item in dataset:
            self.stats["total_original"] += 1
            original_code = item.get("output", "")
            repaired_code = auto_repair(original_code)

            if repaired_code != original_code:
                item["output"] = repaired_code
                item["was_repaired"] = True

            # 1. å§‹ç»ˆä¿ç•™åŸå§‹çš„çœŸé‡‘æ•°æ®
            augmented_dataset.append(item)

            if not original_code:
                continue

            # 2. å°è¯•è§£ææˆ AST
            parse_res = self.parser.get_ast(repaired_code)
            if parse_res.get("status") != "success":
                self.stats["parse_errors"] += 1
                continue

            original_ast = parse_res["ast"]

            # 3. å¾ªç¯ç”Ÿæˆ N ä¸ªå˜ä½“
            for _ in range(self.num_variants):
                try:
                    # æ·±æ‹·è´é˜²æ­¢æ±¡æŸ“
                    ast_clone = copy.deepcopy(original_ast)

                    # å˜å¼‚ä¸åè§£æ
                    mutated_ast = self.rewriter.rewrite(ast_clone)
                    new_code = self.unparser.unparse(mutated_ast)

                    # ç¡®è®¤ä»£ç å‘ç”Ÿäº†å®é™…å˜åŒ– (é˜²é‡å¤)
                    if new_code.strip() and new_code.strip() != original_code.strip():
                        new_item = copy.deepcopy(item)
                        new_item["output"] = new_code
                        new_item["is_augmented"] = True  # æ‰“ä¸Š AI å¢å¼ºæ ‡è®°
                        augmented_dataset.append(new_item)
                        self.stats["total_augmented"] += 1
                except Exception:
                    # å•æ¬¡å¢å¼ºå¤±è´¥ä¸å½±å“æ•´ä½“
                    pass

        return augmented_dataset

    def run(self):
        """æ‰§è¡Œæ‰¹é‡å¢å¼ºæµç¨‹"""
        files = list(self.input_dir.rglob(f"*{self.ext}"))
        self.stats["total_files"] = len(files)

        if not files:
            print(f"âŒ åœ¨ {self.input_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½• {self.ext} æ–‡ä»¶ï¼")
            return

        print(f"ğŸš€ å‘ç° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¯åŠ¨ AST æ‰¹é‡å¢å¼ºå·¥å‚ (è£‚å˜ç³»æ•°: x{self.num_variants})...")

        for file_path in tqdm(files, desc="Augmenting Datasets"):
            augmented_data = self.process_single_file(file_path)

            if not augmented_data:
                continue

            self.stats["processed_files"] += 1

            # ğŸš€ æ ¸å¿ƒæ”¹åŠ¨ï¼šåˆ›å»ºä¸åŸ JSON åŒåçš„æ–‡ä»¶å¤¹
            # æ¯”å¦‚è¾“å…¥æ˜¯ data/golden_prompts.jsonï¼Œå°±ä¼šåœ¨è¾“å‡ºç›®å½•åˆ›å»º golden_prompts/ æ–‡ä»¶å¤¹
            file_out_dir = self.output_dir / file_path.stem
            file_out_dir.mkdir(parents=True, exist_ok=True)

            # å°†å¢å¼ºåçš„æ•°æ®å­˜å…¥è¯¥ä¸“å±æ–‡ä»¶å¤¹
            out_file = file_out_dir / "augmented_golden.json"
            with open(out_file, 'w', encoding='utf-8') as f:
                json.dump(augmented_data, f, ensure_ascii=False, indent=2)

        self.print_report()

    def print_report(self):
        """æ‰“å°ç‚«é…·çš„æµæ°´çº¿æˆ˜æŠ¥"""
        orig = self.stats["total_original"]
        aug = self.stats["total_augmented"]
        err = self.stats["parse_errors"]
        total = orig + aug

        print("\n" + "=" * 55)
        print("ğŸ§¬ AST æ•°æ®é›†å¢å¼ºæµæ°´çº¿æˆ˜æŠ¥")
        print("=" * 55)
        print(f"ğŸ“‚ æ‰«ææ–‡ä»¶æ•°: {self.stats['total_files']} (æˆåŠŸå¤„ç†: {self.stats['processed_files']})")
        print("-" * 55)
        print(f"ğŸŒ± åŸå§‹çœŸé‡‘æ ·æœ¬: {orig:6d} æ¡")
        print(f"ğŸŒ¿ AST è£‚å˜æ ·æœ¬: {aug:6d} æ¡ (å˜å¼‚æˆåŠŸï¼)")
        print(f"âš ï¸ è§£æå¤±è´¥è·³è¿‡: {err:6d} æ¡")
        print("-" * 55)
        print(f"ğŸ“¦ æœ€ç»ˆæ•°æ®æ€»é‡: {total:6d} æ¡")
        print(f"ğŸ“ ç»“æœå·²æŒ‰åŸæ–‡ä»¶ååˆ†å‘è‡³: {self.output_dir.absolute()}")
        print("=" * 55)
