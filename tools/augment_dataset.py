import argparse
import json
import random
import copy
from pathlib import Path
from tqdm import tqdm

from src.stparser.st_parser import STParser
from src.stparser.st_parser import STSemanticAnalyzer
from src.stparser.st_parser import STUnparser
from src.strewriter.st_rewriter import STRewriter


class DataAugmenter:
    def __init__(self):
        self.parser = STParser()
        self.analyzer = STSemanticAnalyzer()
        self.rewriter = STRewriter(analyzer=self.analyzer)
        self.unparser = STUnparser()

    def run(self, input_file: str, output_file: str, variants_per_sample: int = 2):
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½ SFT æ•°æ®é›†: {input_file}")
        with open(input_file, 'r', encoding='utf-8') as f:
            dataset = json.load(f)

        augmented_dataset = []
        success_count = 0
        error_count = 0

        print(f"ğŸš€ å¼€å§‹è¿›è¡Œ AST çº§æ•°æ®å¢å¼º (æ¯æ¡è¡ç”Ÿ {variants_per_sample} ä¸ªå˜ä½“)...")

        for item in tqdm(dataset, desc="Augmenting"):
            original_code = item.get("output", "")
            if not original_code:
                continue

            # 1. åŸæ•°æ®æ— æ¡ä»¶ä¿ç•™
            augmented_dataset.append(item)

            # 2. å°è¯•è§£ææˆ AST
            parse_res = self.parser.get_ast(original_code)
            if parse_res["status"] != "success":
                error_count += 1
                continue

            original_ast = parse_res["ast"]

            # 3. ç”Ÿæˆ N ä¸ªå˜ä½“
            for _ in range(variants_per_sample):
                try:
                    # âš ï¸ å¿…é¡»æ·±æ‹·è´ï¼Œå¦åˆ™ä¼šæ±¡æŸ“åŸ AST
                    ast_clone = copy.deepcopy(original_ast)

                    # AST å˜å¼‚
                    mutated_ast = self.rewriter.rewrite(ast_clone)

                    # åè§£æä¸º ST ä»£ç 
                    new_code = self.unparser.unparse(mutated_ast)

                    # å¦‚æœé‡æ’åä»£ç æœ‰å˜åŒ–ï¼Œå­˜å…¥æ–°æ•°æ®é›†
                    if new_code.strip() != original_code.strip():
                        new_item = copy.deepcopy(item)
                        new_item["output"] = new_code
                        new_item["is_augmented"] = True  # æ‰“ä¸Šå¢å¼ºæ ‡è®°
                        augmented_dataset.append(new_item)
                        success_count += 1
                except Exception as e:
                    # å®¹é”™å¤„ç†ï¼šå³ä½¿æŸä¸ªå˜ä½“ç”Ÿæˆå¤±è´¥ï¼Œä¹Ÿä¸å½±å“æ•´ä¸ªæµæ°´çº¿
                    pass

        # è½ç›˜
        out_path = Path(output_file)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(augmented_dataset, f, ensure_ascii=False, indent=2)

        print("\n" + "=" * 50)
        print("ğŸ‰ æ•°æ®å¢å¼ºå®Œæˆï¼")
        print("=" * 50)
        print(f"åŸå§‹æ•°æ®é‡: {len(dataset)}")
        print(f"æˆåŠŸå¢å¼ºé‡: {success_count} (è§£æå¤±è´¥è·³è¿‡: {error_count})")
        print(f"æœ€ç»ˆæ•°æ®é‡: {len(augmented_dataset)}")
        print(f"ğŸ’¾ å·²ä¿å­˜è‡³: {out_path.absolute()}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="STä»£ç æ•°æ®é›† AST å¢å¼ºå·¥å‚")
    parser.add_argument("-i", "--input", required=True, help="è¾“å…¥çš„ Golden JSON æ•°æ®é›†")
    parser.add_argument("-o", "--output", required=True, help="è¾“å‡ºçš„å¢å¼ºå JSON æ•°æ®é›†")
    parser.add_argument("-n", "--num", type=int, default=2, help="æ¯æ¡åŸå§‹æ•°æ®ç”Ÿæˆçš„å˜ä½“æ•°é‡")
    args = parser.parse_args()

    augmenter = DataAugmenter()
    augmenter.run(args.input, args.output, args.num)