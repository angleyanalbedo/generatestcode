import asyncio
from openai import AsyncOpenAI, APIConnectionError, AuthenticationError


async def test_vllm_connection():
    """
    æµ‹è¯• vLLM æœåŠ¡è¿é€šæ€§
    é»˜è®¤è¿æ¥ http://localhost:8000/v1
    """

    # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆvLLM å…¼å®¹ OpenAI API æ ¼å¼ï¼‰
    client = AsyncOpenAI(
        base_url="http://localhost:8000/v1",  # vLLM é»˜è®¤ç«¯å£å’Œè·¯å¾„
        api_key="not-needed-for-vllm"  # vLLM æœ¬åœ°éƒ¨ç½²é€šå¸¸ä¸éœ€è¦çœŸå® API key
    )

    print("ğŸ” æ­£åœ¨æµ‹è¯• vLLM æœåŠ¡è¿é€šæ€§...")
    print(f"   ç›®æ ‡åœ°å€: http://localhost:8000/v1")
    print("-" * 50)

    try:
        # æµ‹è¯•1: è·å–æ¨¡å‹åˆ—è¡¨ï¼ˆæœ€åŸºç¡€çš„è¿é€šæ€§æµ‹è¯•ï¼‰
        print("\nğŸ“‹ æµ‹è¯•1: è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...")
        models = await client.models.list()
        print(f"   âœ… è¿æ¥æˆåŠŸï¼")
        print(f"   ğŸ“ å¯ç”¨æ¨¡å‹æ•°é‡: {len(models.data)}")
        for model in models.data:
            print(f"      - {model.id}")

        # æµ‹è¯•2: å‘é€ç®€å•çš„ Chat Completion è¯·æ±‚
        print("\nğŸ’¬ æµ‹è¯•2: å‘é€ç®€å•å¯¹è¯è¯·æ±‚...")
        response = await client.chat.completions.create(
            model=models.data[0].id if models.data else "default",  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿é€šæ€§æµ‹è¯•ã€‚è¯·å›å¤'pong'"}],
            max_tokens=10,
            temperature=0
        )
        print(f"   âœ… æ¨ç†æˆåŠŸï¼")
        print(f"   ğŸ“ å“åº”å†…å®¹: {response.choices[0].message.content}")
        print(f"   ğŸ“Š ä½¿ç”¨ token: {response.usage.total_tokens if response.usage else 'N/A'}")

        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼vLLM æœåŠ¡è¿è¡Œæ­£å¸¸")
        return True

    except APIConnectionError as e:
        print(f"\n   âŒ è¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ° vLLM æœåŠ¡")
        print(f"   ğŸ”§ è¯·æ£€æŸ¥:")
        print(f"      1. vLLM æœåŠ¡æ˜¯å¦å·²å¯åŠ¨ (python -m vllm.entrypoints.openai.api_server...)")
        print(f"      2. ç«¯å£ 8000 æ˜¯å¦æ­£ç¡®")
        print(f"      3. é˜²ç«å¢™/ç½‘ç»œè®¾ç½®")
        print(f"   ğŸ“„ é”™è¯¯è¯¦æƒ…: {e}")
        return False

    except AuthenticationError as e:
        print(f"\n   âš ï¸  è®¤è¯é”™è¯¯: {e}")
        print(f"   ğŸ”§ å¦‚æœ vLLM å¯ç”¨äº† API key éªŒè¯ï¼Œè¯·æä¾›æ­£ç¡®çš„ key")
        return False

    except Exception as e:
        print(f"\n   âŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        return False


async def test_streaming():
    """å¯é€‰ï¼šæµ‹è¯•æµå¼è¾“å‡º"""
    client = AsyncOpenAI(
        base_url="http://localhost:8000/v1",
        api_key="industrial-coder"
    )

    print("\nğŸŒŠ é¢å¤–æµ‹è¯•: æµå¼è¾“å‡º...")
    try:
        stream = await client.chat.completions.create(
            model="default",  # æˆ–ä½¿ç”¨å…·ä½“æ¨¡å‹å
            messages=[{"role": "user", "content": "Count: 1,2,3"}],
            stream=True,
            max_tokens=20
        )

        content = ""
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                content += chunk.choices[0].delta.content
                print(chunk.choices[0].delta.content, end="", flush=True)

        print(f"\n   âœ… æµå¼è¾“å‡ºæ­£å¸¸ï¼Œæ”¶åˆ°å†…å®¹: '{content}'")
        return True
    except Exception as e:
        print(f"\n   âŒ æµå¼æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # è¿è¡ŒåŸºç¡€è¿é€šæ€§æµ‹è¯•
    connected = asyncio.run(test_vllm_connection())

    # å¦‚æœåŸºç¡€æµ‹è¯•é€šè¿‡ï¼Œå¯é€‰è¿è¡Œæµå¼æµ‹è¯•
    if connected:
        asyncio.run(test_streaming())